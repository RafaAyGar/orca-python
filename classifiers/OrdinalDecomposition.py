
from sys import path
path.insert(0, '../')

import numpy as np
from sklearn.metrics.scorer import make_scorer
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn.utils.validation import check_X_y, check_array, check_is_fitted

from utilities import load_classifier


class OrdinalDecomposition(BaseEstimator, ClassifierMixin):

	"""
	OrdinalDecomposition ensemble classifier

	This class implements an ensemble method, where an ordinal problem is 
	decomposed into several binary subproblems, each one of which will
	generate a different model, though all will share same inner classifier
	and parameters for it.

	There are 4 different ways to decompose the original problem based on 
	how the coding matrix is built.


	Parameters
	----------

	dtype: string
		Type of decomposition to be performed by class. May be one of 4 different
		types: 'OrderedPartitions', 'OneVsNext', 'OneVsFollowers' or 'OneVsPrevious'

		The coding matrix generated by each method, for a problem with 5 classes
		will be as follows:

		OrderedPartitions	OneVsNext		OneVsFollowers		OneVsPrevious
		
			-, -, -, -;		-,  ,  ,  ;		-,  ,  ,  ;		+, +, +, +;
			+, -, -, -;		+, -,  ,  ;		+, -,  ,  ;		+, +, +, -;
			+, +, -, -;		 , +, -,  ;		+, +, -,  ;		+, +, -,  ;
			+, +, +, -;		 ,  , +, -;		+, +, +, -;		+, -,  ,  ;
			+, +, +, +;		 ,  ,  , +;		+, +, +, +;		-,  ,  ,  ;

		where rows represents classes and columns classifiers. plus signs indicate
		that for that classifier, that class will be part of the positive class,
		on the other hand, a minus sign places that class into the the negative one
		for that binary problem. If there is no sign, then those samples will not be
		taken into account when building the model.

	decision_method: string
		Decision method used to transform from n different classifier predictions to
		give the final label (one among the real classes) to a given set.

	classifier: string
		Inner classifier to be used to build a model for each binary subproblem.
		It has to call a local classifier built into this framework, or a class
		from scikit-learn.

	parameters: dict
		This dictionary will contain the parameters which the inner classifier 
		will be built with. As this class has been created so that cross-validation
		is carried out outside it, only one value per parameter is allowed.


	Attributes
	----------

	classes_: list
		List that contains all different class labels found in original dataset


	coding_matrix_: array-like, shape (n_targets, n_targets-1)
		Matrix that defines which classes use when building each subproblem, 
		and in which binary class they belong inside those new models.

		Further explained previously.

	classifiers_: list of classifiers
		Initialy empty, will include all fitted models for each subproblem
		once the fit function for this class is called successfully.


	References
	----------



	"""

	#TODO: Especificar valores por defecto
	def __init__(self, dtype="", decision_method="", base_classifier="",  parameters={}):

		self.dtype = dtype
		self.decision_method = decision_method
		self.base_classifier = base_classifier
		self.parameters = parameters


	def fit(self, X, y):

		"""
		Fit the model according to the given training data

		Parameters
		----------

		X: {arra-like, sparse matrix}, shape (n_samples, n_features)
			Training vector, where n_samples is the number of samples and
			n_features is the number of features

		y: array-like, shape (n_samples)
			Target vector relative to X

		Returns
		-------

		self: object
		"""


		X, y = check_X_y(X, y)

		self.X_ = X
		self.y_ = y

		# Get list of different targets of dataset
		self.classes_ = np.unique(y)

		# Gives each train input its corresponding output label for each binary classifier
		self.coding_matrix_ = self._coding_matrix( len(self.classes_) )
		class_labels = self.coding_matrix_[ (np.digitize(y, self.classes_) - 1), :]


		self.classifiers_ = []
		# Fitting n_targets-1 classifiers, each one with a different combination of train inputs
		# given by the coding_matrix
		for n in range(len(class_labels[0,:])):

			estimator = load_classifier(self.base_classifier, self.parameters)
			estimator.fit(X[ np.where(class_labels[:,n] != 0) ], \
						  np.ravel(class_labels[np.where(class_labels[:,n] != 0), n].T) )


			self.classifiers_.append(estimator)


		return self


	def predict(self, X):

		"""
		Performs classification on samples in X

		Parameters
		----------

		X : {array-like, sparse matrix}, shape (n_samples, n_features)

		Returns
		-------

		predicted_y : array, shape (n_samples,)
			Class labels for samples in X.
		"""

		# Check is fit had been called
		check_is_fitted(self, ['X_', 'y_'])

		# Input validation
		X = check_array(X)


		decision_method = self.decision_method.lower()

		if decision_method == "exponential_loss":

			losses = self._exponential_loss(X)
			predicted_y = self.classes_[np.argmin(losses, axis=1)]


		elif decision_method == "hinge_loss":
			
			losses = self._hinge_loss(X)
			predicted_y = self.classes_[np.argmin(losses, axis=1)]


		elif decision_method == "logaritmic_loss":

			losses = self._logaritmic_loss(X)
			predicted_y = self.classes_[np.argmin(losses, axis=1)]


		elif decision_method == "frank_hall":

			predicted_proba_y = self._frank_hall_method(X)
			predicted_y = self.classes_[np.argmax(predicted_proba_y, axis=1)]


		else:
			raise AttributeError('The specified loss method "%s" is not implemented' % decision_method)


		return predicted_y






	def _coding_matrix(self, n_classes):

		"""
		Method that returns the coding matrix for a given dataset.

		Parameters
		----------

		n_classes: int
			Number of different classes in actual dataset
		
		dtype: string
			Type of decomposition to be applied

		Returns
		-------

		coding_matrix: array-like, shape (n_targets, n_targets-1)
			Each value must be in range {-1, 1, 0}, whether that class
		 	will belong to negative class, positive class or will not be 
			used for that particular binary classifier.
		"""

		dtype = self.dtype.lower()
		if dtype == "ordered_partitions":

			coding_matrix = np.triu( (-2 * np.ones(n_classes - 1)) ) + 1
			coding_matrix = np.vstack([coding_matrix, np.ones((1, n_classes-1))])

		elif dtype == "one_vs_next":

			plus_ones = np.diagflat(np.ones((1, n_classes - 1), dtype=int), -1)
			minus_ones = -( np.eye(n_classes, n_classes - 1, dtype=int) )
			coding_matrix = minus_ones + plus_ones[:,:-1]

		elif dtype == "one_vs_followers":

			minus_ones = np.diagflat(-np.ones((1, n_classes), dtype=int))
			plus_ones = np.tril(np.ones(n_classes), -1)
			coding_matrix = (plus_ones + minus_ones)[:,:-1]

		elif dtype == "one_vs_previous":

			plusones = np.triu(np.ones(n_classes))
			minusones = -np.diagflat(np.ones((1, n_classes - 1)), -1)
			coding_matrix = np.flip( (plusones + minusones)[:,:-1], axis=1 )

		else:

			raise ValueError("Decomposition type %s does not exist" % dtype)

		return coding_matrix.astype(int)




	def _exponential_loss(self, X):

		"""
		Computation of the exponential losses for each label of the
		original ordinal multinomial problem. Transforms fron n 
		binary subproblems to the origial ordinal problem.

		Parameters
		----------

		X : {array-like, sparse matrix}, shape (n_samples, n_features)

		Returns
		-------

		e_losses : array, shape (n_samples, unique_labels)
			Exponential losses for each sample of dataset X. One different
			value for each class label.
		"""


		positive_class = 1
		# Mapping decission probabilities for possitive class from [0 ~ 1] range to [-1 ~ 1]
		predictions = np.array([ ( np.ravel(c.predict_proba(X)[:, np.where(c.classes_ == positive_class)])*2 ) - 1
									 for c in self.classifiers_]).T


		# Computing exponential losses
		e_losses = np.zeros( (X.shape[0], self.coding_matrix_.shape[0]) )
		for i in range(self.coding_matrix_.shape[0]):

			e_losses[:,i] = np.sum(np.exp( -predictions * np.tile(self.coding_matrix_[i,:], (predictions.shape[0], 1)) ), axis=1)

		return e_losses




	def _hinge_loss(self, X):

		"""
		Computation of the Hinge losses for each label of the
		original ordinal multinomial problem. Transforms fron n 
		binary subproblems to the origial ordinal problem.

		Parameters
		----------

		X : {array-like, sparse matrix}, shape (n_samples, n_features)

		Returns
		-------

		hLosses : array, shape (n_samples, unique_labels)
			Hinge losses for each sample of dataset X. One different
			value for each class label.

		"""

		# Mapping decission probabilities for possitive class from [0 ~ 1] range to [-1 ~ 1]
		predictions = np.array([ ( np.ravel(c.predict_proba(X)[:, np.where(c.classes_ == 1)])*2 ) - 1
									 for c in self.classifiers_]).T

		# Computing Hinge losses
		h_losses = np.zeros( (X.shape[0], self.coding_matrix_.shape[0]) )
		for i in range(self.coding_matrix_.shape[0]):

			h_losses[:,i] = np.sum( np.maximum(0, (1 - np.tile(self.coding_matrix_[i,:], (predictions.shape[0], 1)) * predictions) ), axis=1 )

		return h_losses



	def _logaritmic_loss(self, X):

		"""
		Computation of the logaritmic losses for each label of the
		original ordinal multinomial problem. Transforms fron n 
		binary subproblems to the origial ordinal problem again.

		Parameters
		----------

		X : {array-like, sparse matrix}, shape (n_samples, n_features)

		Returns
		-------

		eLosses : array, shape (n_samples, unique_labels)
			Logaritmic losses for each sample of dataset X. One different
			value for each class label

		"""

		# Mapping decission probabilities for possitive class from [0 ~ 1] range to [-1 ~ 1]
		predictions = np.array([ ( np.ravel(c.predict_proba(X)[:, np.where(c.classes_ == 1)])*2 ) - 1
									 for c in self.classifiers_]).T

		# Computing logaritmic losses
		l_losses = np.zeros( (X.shape[0], self.coding_matrix_.shape[0]) )
		for i in range(self.coding_matrix_.shape[0]):

			l_losses[:,i] = np.sum( np.log(1 + np.exp(-2 * np.tile(self.coding_matrix_[i,:], (predictions.shape[0], 1)) * predictions)), axis=1 )


		return l_losses



	def _frank_hall_method(self, X):

		"""
		Decision method used to transform from n predictions of binary
		problems to predictions of a multinomial ordinal classification

		Parameters
		----------

		X : {array-like, sparse matrix}, shape (n_samples, n_features)

		Returns
		-------

		predicted_y : array, shape (n_samples,)
			Class labels for samples in X.
		"""


		if self.dtype.lower() != "ordered_partitions":
			raise AttributeError("When using Frank and Hall decision method, ordered_partitions must be used")



		predicted_proba_y = np.empty( [X.shape[0], len(self.classifiers_) + 1] )
		positive_class_placement = np.where(self.classifiers_[0].classes_ == 1)


		# Probabilities of each set to belong to the first ordinal class
		predicted_proba_y[:,0] = 1 - np.ravel( self.classifiers_[0].predict_proba(X)[:, positive_class_placement] )

		for i, c in enumerate(self.classifiers_[1:], 1):

			# Prbability of sets to belong to class i
			predicted_proba_y[:,i] = np.ravel( self.classifiers_[i-1].predict_proba(X)[:, positive_class_placement] ) - \
									np.ravel(self.classifiers_[i].predict_proba(X)[:, positive_class_placement])

		# Probabilities of each set to belong to the last class
		predicted_proba_y[:,-1] = np.ravel( self.classifiers_[-1].predict_proba(X)[:, positive_class_placement] )


		return predicted_proba_y






